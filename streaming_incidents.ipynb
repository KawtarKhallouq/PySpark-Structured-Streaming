{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hospital Incident Data Analysis with PySpark Structured Streaming\n",
    "## Project Overview\n",
    "This notebook demonstrates how to analyze streaming hospital incident data using PySpark’s Structured Streaming. Hospitals generate large volumes of data daily, including records of various incidents across departments. Analyzing this data in real-time can help hospital management quickly identify high-incident departments and take preventative actions or allocate resources more efficiently.\n",
    "\n",
    "## Objectives\n",
    "In this analysis, we aim to:\n",
    "\n",
    "* Continuously monitor and display the number of incidents by department (e.g., Cardiology, Neurology).\n",
    "* Identify and display the two years with the highest number of incidents, providing insights into trends over time.\n",
    "## Data Format\n",
    "The data is streamed as CSV files with the following structure:\n",
    "\n",
    "* Id: Unique identifier for each incident.\n",
    "* Title: Brief title of the incident.\n",
    "* Description: Detailed description of the incident.\n",
    "* Service: Hospital department where the incident occurred.\n",
    "* Date: Date of the incident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialize Spark Session and Set Up Streaming Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, col, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"HospitalIncidents\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure the static data folder exists\n",
    "static_data_folder = \"../data/static/\"\n",
    "os.makedirs(static_data_folder, exist_ok=True)\n",
    "\n",
    "# Sample data for the CSV file\n",
    "data = {\n",
    "    \"Id\": [1, 2, 3, 4, 5],\n",
    "    \"titre\": [\"Incident 1\", \"Incident 2\", \"Incident 3\", \"Incident 4\", \"Incident 5\"],\n",
    "    \"description\": [\"Description 1\", \"Description 2\", \"Description 3\", \"Description 4\", \"Description 5\"],\n",
    "    \"service\": [\"Emergency\", \"Pediatrics\", \"Radiology\", \"Emergency\", \"Pediatrics\"],\n",
    "    \"date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\", \"2023-01-05\"]\n",
    "}\n",
    "\n",
    "# Create a DataFrame and save it as a CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(os.path.join(static_data_folder, \"incoming.csv\"), index=False)\n",
    "print(\"Static CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Incidents by Service in Real-Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a Python script or notebook cell to copy incoming.csv into the  folder at intervals, so that Spark detects it as a new file each time.\n",
    "This way, the Structured Streaming application can keep processing each new \"streaming\" event as if they’re fresh records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file exists: ../data/static/incoming.csv\n",
      "Target folder exists: ../data/incoming/\n",
      "Copied file to ../data/incoming/incoming_72bf1108-ccfa-4987-b8a9-18365efbe9c6.csv\n",
      "Confirmed: ../data/incoming/incoming_72bf1108-ccfa-4987-b8a9-18365efbe9c6.csv exists.\n",
      "Copied file to ../data/incoming/incoming_d5dd9553-2a1c-4544-8e51-2553de009401.csv\n",
      "Confirmed: ../data/incoming/incoming_d5dd9553-2a1c-4544-8e51-2553de009401.csv exists.\n",
      "Copied file to ../data/incoming/incoming_bcef30fc-736d-4934-b7fd-925a4184116e.csv\n",
      "Confirmed: ../data/incoming/incoming_bcef30fc-736d-4934-b7fd-925a4184116e.csv exists.\n",
      "Copied file to ../data/incoming/incoming_b19f733c-d169-42c1-b8b5-6100b1550a4f.csv\n",
      "Confirmed: ../data/incoming/incoming_b19f733c-d169-42c1-b8b5-6100b1550a4f.csv exists.\n",
      "Copied file to ../data/incoming/incoming_bd2949e1-d6bb-470b-85e0-ed0dedd920eb.csv\n",
      "Confirmed: ../data/incoming/incoming_bd2949e1-d6bb-470b-85e0-ed0dedd920eb.csv exists.\n",
      "Copied file to ../data/incoming/incoming_5079505b-cf3a-4915-a332-192a9527021a.csv\n",
      "Confirmed: ../data/incoming/incoming_5079505b-cf3a-4915-a332-192a9527021a.csv exists.\n",
      "Copied file to ../data/incoming/incoming_1291087f-5a3b-4a53-adf9-994fef0874e0.csv\n",
      "Confirmed: ../data/incoming/incoming_1291087f-5a3b-4a53-adf9-994fef0874e0.csv exists.\n",
      "Copied file to ../data/incoming/incoming_285e65cd-ce41-44d1-8e64-de06dc9a2e33.csv\n",
      "Confirmed: ../data/incoming/incoming_285e65cd-ce41-44d1-8e64-de06dc9a2e33.csv exists.\n",
      "Copied file to ../data/incoming/incoming_3314742d-95dc-4314-9698-64b00d562c81.csv\n",
      "Confirmed: ../data/incoming/incoming_3314742d-95dc-4314-9698-64b00d562c81.csv exists.\n",
      "Copied file to ../data/incoming/incoming_100c38b4-a5e8-4d27-975c-3ef15117d970.csv\n",
      "Confirmed: ../data/incoming/incoming_100c38b4-a5e8-4d27-975c-3ef15117d970.csv exists.\n",
      "Files in target directory after copying:\n",
      "['incoming_08bcd605-2d92-4fc9-a7b8-80fca4c23664.csv', 'incoming_100c38b4-a5e8-4d27-975c-3ef15117d970.csv', 'incoming_1291087f-5a3b-4a53-adf9-994fef0874e0.csv', 'incoming_21de8d84-98ce-4b22-bf11-48e34826051a.csv', 'incoming_2654a857-6e95-497d-b7b7-9e74b8d5893f.csv', 'incoming_285e65cd-ce41-44d1-8e64-de06dc9a2e33.csv', 'incoming_3314742d-95dc-4314-9698-64b00d562c81.csv', 'incoming_3d1ee6aa-8d87-411c-9259-7b959e586fe9.csv', 'incoming_3ec07362-587b-4dbd-9b5a-2ee260f7202b.csv', 'incoming_3ee6ed7d-e255-427f-aec4-1d9b5e9bcd7d.csv', 'incoming_43b97eba-4989-4394-aa7a-29156472cf4a.csv', 'incoming_4f564721-c38e-45e6-bd9c-d64b22765fa4.csv', 'incoming_5079505b-cf3a-4915-a332-192a9527021a.csv', 'incoming_55db5e54-eeed-4eb2-8e35-6d719a2a1433.csv', 'incoming_55f9a30f-d75b-48bd-93b9-d9e338e35cf1.csv', 'incoming_577c068b-9843-474e-978a-3e2269010e82.csv', 'incoming_6946161a-0d35-4a94-b120-c121509dc25a.csv', 'incoming_72bf1108-ccfa-4987-b8a9-18365efbe9c6.csv', 'incoming_9dafbde4-8c32-49a4-9124-1db44ec95de1.csv', 'incoming_a4dc4d2f-9ca3-4331-aff1-f5a65da52f7e.csv', 'incoming_b19f733c-d169-42c1-b8b5-6100b1550a4f.csv', 'incoming_bcef30fc-736d-4934-b7fd-925a4184116e.csv', 'incoming_bd2949e1-d6bb-470b-85e0-ed0dedd920eb.csv', 'incoming_cb0ece3e-454c-48a1-99d1-f9273444ada9.csv', 'incoming_d5dd9553-2a1c-4544-8e51-2553de009401.csv', 'incoming_e2881e95-3c16-4075-b179-4e195c254aaf.csv', 'incoming_eab3b6b1-8230-4061-9a1a-4af70b8c99be.csv', 'incoming_ebb41cba-c254-4d90-aa64-b7f7cd0c4968.csv', 'incoming_f1b2b4f2-dbdb-4052-99e9-621f016ee41b.csv', 'incoming_fd5bce25-05aa-4969-8810-7e1abebff43a.csv']\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the source and target paths\n",
    "source_file = \"../data/static/incoming.csv\"  # Your static CSV file\n",
    "target_folder = \"../data/incoming/\"  # Folder to copy files to\n",
    "\n",
    "\n",
    "\n",
    "# Check if the source file exists\n",
    "if not os.path.exists(source_file):\n",
    "    print(f\"Source file does not exist: {source_file}\")\n",
    "else:\n",
    "    print(f\"Source file exists: {source_file}\")\n",
    "\n",
    "# Ensure target folder exists\n",
    "if not os.path.exists(target_folder):\n",
    "    print(f\"Target folder does not exist: {target_folder}\")\n",
    "else:\n",
    "    print(f\"Target folder exists: {target_folder}\")\n",
    "\n",
    "# Copy files to the target directory\n",
    "for _ in range(10):  # Adjust the range for more iterations if needed\n",
    "    target_file = os.path.join(target_folder, f\"incoming_{uuid.uuid4()}.csv\")\n",
    "    try:\n",
    "        shutil.copy(source_file, target_file)\n",
    "        print(f\"Copied file to {target_file}\")\n",
    "        \n",
    "        # Check if the file exists after copying\n",
    "        if os.path.exists(target_file):\n",
    "            print(f\"Confirmed: {target_file} exists.\")\n",
    "        else:\n",
    "            print(f\"Warning: {target_file} does not exist after copying.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error copying file: {e}\")\n",
    "    \n",
    "    time.sleep(1)  # Wait 1 second before copying the next file\n",
    "\n",
    "# Final check of files in the target directory\n",
    "print(\"Files in target directory after copying:\")\n",
    "print(os.listdir(target_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"titre\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"service\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)  # You can use DateType() if you will convert the date later\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o237.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:265)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:259)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:36)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:198)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:212)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.csv(DataStreamReader.scala:260)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LENOVO\\Desktop\\S5\\Spark Structured Streaming\\streaming_incidents.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/S5/Spark%20Structured%20Streaming/streaming_incidents.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m incidents_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mreadStream\u001b[39m.\u001b[39;49mschema(schema) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/S5/Spark%20Structured%20Streaming/streaming_incidents.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                  \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/S5/Spark%20Structured%20Streaming/streaming_incidents.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                  \u001b[39m.\u001b[39;49mcsv(target_folder)  \u001b[39m# This should point to the directory\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/S5/Spark%20Structured%20Streaming/streaming_incidents.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Example aggregation: Count incidents by service\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Desktop/S5/Spark%20Structured%20Streaming/streaming_incidents.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m incidents_by_service \u001b[39m=\u001b[39m incidents_df\u001b[39m.\u001b[39mgroupBy(\u001b[39m\"\u001b[39m\u001b[39mservice\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcount()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:715\u001b[0m, in \u001b[0;36mDataStreamReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[0;32m    683\u001b[0m     schema\u001b[39m=\u001b[39mschema,\n\u001b[0;32m    684\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    712\u001b[0m     unescapedQuoteHandling\u001b[39m=\u001b[39munescapedQuoteHandling,\n\u001b[0;32m    713\u001b[0m )\n\u001b[0;32m    714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 715\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(path))\n\u001b[0;32m    716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    718\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_STR\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    719\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(path)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    720\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o237.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:265)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:259)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:36)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:198)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:212)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.csv(DataStreamReader.scala:260)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "incidents_df = spark.readStream.schema(schema) \\\n",
    "                                 .option(\"header\", \"true\") \\\n",
    "                                 .csv(target_folder)  # This should point to the directory\n",
    "\n",
    "\n",
    "# Example aggregation: Count incidents by service\n",
    "incidents_by_service = incidents_df.groupBy(\"service\").count()\n",
    "\n",
    "# Start the query to continuously output counts to the console\n",
    "query_service_count = incidents_by_service.writeStream \\\n",
    "                                           .outputMode(\"complete\") \\\n",
    "                                           .format(\"console\") \\\n",
    "                                           .option(\"truncate\", \"false\") \\\n",
    "                                           .start()\n",
    "\n",
    "query_service_count.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in incoming directory: ['incoming_08bcd605-2d92-4fc9-a7b8-80fca4c23664.csv', 'incoming_100c38b4-a5e8-4d27-975c-3ef15117d970.csv', 'incoming_1291087f-5a3b-4a53-adf9-994fef0874e0.csv', 'incoming_21de8d84-98ce-4b22-bf11-48e34826051a.csv', 'incoming_2654a857-6e95-497d-b7b7-9e74b8d5893f.csv', 'incoming_285e65cd-ce41-44d1-8e64-de06dc9a2e33.csv', 'incoming_3314742d-95dc-4314-9698-64b00d562c81.csv', 'incoming_3d1ee6aa-8d87-411c-9259-7b959e586fe9.csv', 'incoming_3ec07362-587b-4dbd-9b5a-2ee260f7202b.csv', 'incoming_3ee6ed7d-e255-427f-aec4-1d9b5e9bcd7d.csv', 'incoming_43b97eba-4989-4394-aa7a-29156472cf4a.csv', 'incoming_4f564721-c38e-45e6-bd9c-d64b22765fa4.csv', 'incoming_5079505b-cf3a-4915-a332-192a9527021a.csv', 'incoming_55db5e54-eeed-4eb2-8e35-6d719a2a1433.csv', 'incoming_55f9a30f-d75b-48bd-93b9-d9e338e35cf1.csv', 'incoming_577c068b-9843-474e-978a-3e2269010e82.csv', 'incoming_6946161a-0d35-4a94-b120-c121509dc25a.csv', 'incoming_72bf1108-ccfa-4987-b8a9-18365efbe9c6.csv', 'incoming_9dafbde4-8c32-49a4-9124-1db44ec95de1.csv', 'incoming_a4dc4d2f-9ca3-4331-aff1-f5a65da52f7e.csv', 'incoming_b19f733c-d169-42c1-b8b5-6100b1550a4f.csv', 'incoming_bcef30fc-736d-4934-b7fd-925a4184116e.csv', 'incoming_bd2949e1-d6bb-470b-85e0-ed0dedd920eb.csv', 'incoming_cb0ece3e-454c-48a1-99d1-f9273444ada9.csv', 'incoming_d5dd9553-2a1c-4544-8e51-2553de009401.csv', 'incoming_e2881e95-3c16-4075-b179-4e195c254aaf.csv', 'incoming_eab3b6b1-8230-4061-9a1a-4af70b8c99be.csv', 'incoming_ebb41cba-c254-4d90-aa64-b7f7cd0c4968.csv', 'incoming_f1b2b4f2-dbdb-4052-99e9-621f016ee41b.csv', 'incoming_fd5bce25-05aa-4969-8810-7e1abebff43a.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "incoming_dir = \"../data/incoming/\"\n",
    "print(\"Files in incoming directory:\", os.listdir(incoming_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+----------+----------+\n",
      "| Id|     titre|  description|   service|      date|\n",
      "+---+----------+-------------+----------+----------+\n",
      "|  1|Incident 1|Description 1| Emergency|2023-01-01|\n",
      "|  2|Incident 2|Description 2|Pediatrics|2023-01-02|\n",
      "|  3|Incident 3|Description 3| Radiology|2023-01-03|\n",
      "|  4|Incident 4|Description 4| Emergency|2023-01-04|\n",
      "|  5|Incident 5|Description 5|Pediatrics|2023-01-05|\n",
      "+---+----------+-------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"../data/incoming/incoming_100c38b4-a5e8-4d27-975c-3ef15117d970.csv\"  # Replace with an actual file\n",
    "test_df = spark.read.schema(schema).option(\"header\", \"true\").csv(test_file_path)\n",
    "test_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
